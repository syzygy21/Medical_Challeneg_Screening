{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89407e0d-e51d-40f7-b2ce-18fe81409ac0",
   "metadata": {},
   "source": [
    "Medical Question-Answering Model Training Pipeline\n",
    "---------------------------------------------------\n",
    "This notebook implements a fine-tuning pipeline for a medical question-answering system\n",
    "using the Flan-T5 model from Google. The model is trained to generate accurate and \n",
    "relevant responses to medical questions based on a provided dataset.\n",
    "\n",
    "Author: Navdeep\n",
    "Last Modified: April 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7121a237-d20d-4f8a-87fb-b729d4e27ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. ENVIRONMENT AND LIBRARY SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Fix potential OpenMP runtime library conflicts\n",
    "\n",
    "# Fix potential OpenMP runtime library conflicts\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Import core libraries\n",
    "import pandas as pd  # For data manipulation\n",
    "import torch  # For deep learning operations\n",
    "import numpy as np  # For numerical computation\n",
    "\n",
    "# Import HuggingFace Transformers libraries for fine-tuning\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,  # For loading pre-trained seq2seq models\n",
    "    AutoTokenizer,  # For tokenizing text\n",
    "    Seq2SeqTrainingArguments,  # For configuring training parameters\n",
    "    Seq2SeqTrainer,  # For managing the training process\n",
    "    DataCollatorForSeq2Seq,  # For batching and padding sequences\n",
    "    EarlyStoppingCallback  # For stopping training when performance plateaus\n",
    ")\n",
    "\n",
    "# Import dataset and evaluation utilities\n",
    "from datasets import Dataset  # For creating and managing datasets\n",
    "import evaluate  # For loading evaluation metrics\n",
    "import nltk  # For natural language processing\n",
    "from nltk.tokenize import sent_tokenize  # For sentence tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4d5efc-7d71-4034-9bc0-3ca7d5278834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. DATA LOADING AND PREPROCESSING\n",
    "# ============================================================================\n",
    "import re  # For text cleaning with regular expressions\n",
    "\n",
    "# Load the medical question-answer dataset from CSV\n",
    "df = pd.read_csv(r\"C:\\Users\\Navdeep\\Documents\\ML_Challenge\\mle_screening_dataset.csv\")\n",
    "\n",
    "# Define a function to clean special formatting and unwanted content from text\n",
    "def clean_special_terms(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing URLs, email addresses, HTML tags, and normalizing whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text (str or None): The text to clean\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned text\n",
    "    \"\"\"\n",
    "    # Handle non-string inputs by returning empty string\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Normalize whitespace (remove extra spaces, tabs, newlines)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the answers\n",
    "df[\"answer\"] = df[\"answer\"].apply(clean_special_terms)\n",
    "\n",
    "# Clean questions and handle missing values\n",
    "df[\"question\"] = df[\"question\"].str.strip()  # Remove leading/trailing whitespace\n",
    "df[\"answer\"] = df[\"answer\"].fillna(\"\").str.strip()  # Replace NaN with empty string\n",
    "\n",
    "# Rename columns to match the expected format for seq2seq models\n",
    "df = df.rename(columns={\"question\": \"input_text\", \"answer\": \"target_text\"})\n",
    "\n",
    "# Convert pandas DataFrame to HuggingFace Dataset format for easier processing\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split the dataset into training (90%) and testing (10%) sets\n",
    "# Fixed random seed (42) ensures reproducibility\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0608e86b-cb3a-4eea-a6da-3e52630b6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. MODEL AND TOKENIZER INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "# Select the pre-trained model - Flan-T5 small is a good balance of size and performance\n",
    "model_name = \"google/flan-t5-small\"  # Smaller model to fit within GPU constraints\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "708ec3d4-a4d3-43dc-92e8-d48459caa186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Navdeep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Navdeep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d776c1d32d4185a885cdcc88ee76fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14765 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5740f689d9466095a83459d290a1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. TEXT PREPROCESSING AND TOKENIZATION\n",
    "# ============================================================================\n",
    "\n",
    "# Download NLTK resources for sentence tokenization\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Define a function to preprocess and tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Prepares examples for the model by adding a task-specific prefix and tokenizing.\n",
    "    \n",
    "    Args:\n",
    "        examples (dict): Batch of examples with input_text and target_text fields\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processed examples with input_ids, attention_mask, and labels\n",
    "    \"\"\"\n",
    "    # Add a task-specific prefix to help the model understand the task\n",
    "    prefix = \"Answer the medical question: \"\n",
    "    inputs = [prefix + question for question in examples[\"input_text\"]]\n",
    "    \n",
    "    # Tokenize inputs (questions) with truncation at 512 tokens\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    \n",
    "    # Tokenize targets (answers) with truncation at 128 tokens\n",
    "    labels = tokenizer(text_target=examples[\"target_text\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to both training and test datasets\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Load the pre-trained T5 model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf92c2f-1e0e-43c6-b52f-fc424f91120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Configure training parameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=r\"C:\\Users\\Navdeep\\Documents\\ML_Challenge\\flan-t5-medical-qa_3\",  # Directory to save model checkpoints\n",
    "    eval_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",  # Save checkpoint after each epoch\n",
    "    learning_rate=3e-5,  # Learning rate (adjusted for fine-tuning)\n",
    "    per_device_train_batch_size=8,  # Batch size for training\n",
    "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
    "    weight_decay=0.01,  # L2 regularization to prevent overfitting\n",
    "    save_total_limit=3,  # Keep only the 3 best checkpoints to save disk space\n",
    "    num_train_epochs=20,  # Maximum number of training epochs\n",
    "    predict_with_generate=True,  # Use generation for evaluation (needed for ROUGE)\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model='eval_rougeL'  # Metric to track for best model selection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "febe8f0a-f3f2-4d67-ab66-e3c122dd1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. DATA COLLATION AND METRICS SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Define data collator for padding batches to the same length\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,  # Ignore padding tokens in loss calculation\n",
    "    pad_to_multiple_of=8 if training_args.fp16 else None,  # Optimize for GPU efficiency\n",
    ")\n",
    "\n",
    "# Load the ROUGE metric for evaluation\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# Define a function to compute evaluation metrics\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"\n",
    "    Compute ROUGE scores between predictions and references.\n",
    "    \n",
    "    Args:\n",
    "        eval_preds (tuple): Tuple containing predictions and labels\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    # Replace -100 padding tokens with the tokenizer's pad token for decoding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode the predictions and labels back into text\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Format for ROUGE: expects newlines between sentences\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, \n",
    "        references=decoded_labels, \n",
    "        use_stemmer=True  # Use stemming for better matching\n",
    "    )\n",
    "    \n",
    "    # Convert scores to percentages for easier interpretation\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length as an additional metric\n",
    "    prediction_lens = [len(pred.split()) for pred in decoded_preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    # Round values for cleaner output\n",
    "    return {k: round(v, 4) for k, v in result.items()}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5b3dee7-6694-4cb4-86bf-74e1b4335045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Navdeep\\AppData\\Local\\Temp\\ipykernel_10172\\2407036359.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35074' max='36920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35074/36920 2:23:13 < 07:32, 4.08 it/s, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.395100</td>\n",
       "      <td>2.055750</td>\n",
       "      <td>20.078000</td>\n",
       "      <td>12.184100</td>\n",
       "      <td>18.253600</td>\n",
       "      <td>19.189500</td>\n",
       "      <td>12.081700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.207700</td>\n",
       "      <td>1.962967</td>\n",
       "      <td>20.893600</td>\n",
       "      <td>12.903800</td>\n",
       "      <td>19.038000</td>\n",
       "      <td>19.982700</td>\n",
       "      <td>12.174900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.119000</td>\n",
       "      <td>1.909121</td>\n",
       "      <td>20.799300</td>\n",
       "      <td>12.877200</td>\n",
       "      <td>18.937200</td>\n",
       "      <td>19.888300</td>\n",
       "      <td>12.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.101900</td>\n",
       "      <td>1.870995</td>\n",
       "      <td>20.915200</td>\n",
       "      <td>12.989100</td>\n",
       "      <td>19.100300</td>\n",
       "      <td>20.040300</td>\n",
       "      <td>12.154200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.021600</td>\n",
       "      <td>1.842130</td>\n",
       "      <td>21.043200</td>\n",
       "      <td>13.176600</td>\n",
       "      <td>19.230300</td>\n",
       "      <td>20.140900</td>\n",
       "      <td>12.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.995400</td>\n",
       "      <td>1.820188</td>\n",
       "      <td>21.028000</td>\n",
       "      <td>13.140400</td>\n",
       "      <td>19.194800</td>\n",
       "      <td>20.112600</td>\n",
       "      <td>12.185300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.969800</td>\n",
       "      <td>1.802049</td>\n",
       "      <td>21.171900</td>\n",
       "      <td>13.187300</td>\n",
       "      <td>19.324400</td>\n",
       "      <td>20.237900</td>\n",
       "      <td>12.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.959400</td>\n",
       "      <td>1.785552</td>\n",
       "      <td>21.147600</td>\n",
       "      <td>13.232500</td>\n",
       "      <td>19.313200</td>\n",
       "      <td>20.224900</td>\n",
       "      <td>12.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.932400</td>\n",
       "      <td>1.773280</td>\n",
       "      <td>21.229400</td>\n",
       "      <td>13.307700</td>\n",
       "      <td>19.378100</td>\n",
       "      <td>20.329200</td>\n",
       "      <td>12.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.920200</td>\n",
       "      <td>1.762884</td>\n",
       "      <td>21.421200</td>\n",
       "      <td>13.360200</td>\n",
       "      <td>19.539900</td>\n",
       "      <td>20.488700</td>\n",
       "      <td>12.263900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.894400</td>\n",
       "      <td>1.753956</td>\n",
       "      <td>21.367700</td>\n",
       "      <td>13.388600</td>\n",
       "      <td>19.526700</td>\n",
       "      <td>20.471700</td>\n",
       "      <td>12.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.898900</td>\n",
       "      <td>1.745752</td>\n",
       "      <td>21.398900</td>\n",
       "      <td>13.479800</td>\n",
       "      <td>19.596700</td>\n",
       "      <td>20.502100</td>\n",
       "      <td>12.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.889000</td>\n",
       "      <td>1.740004</td>\n",
       "      <td>21.403500</td>\n",
       "      <td>13.508200</td>\n",
       "      <td>19.597500</td>\n",
       "      <td>20.524300</td>\n",
       "      <td>12.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.871300</td>\n",
       "      <td>1.735352</td>\n",
       "      <td>21.487700</td>\n",
       "      <td>13.595600</td>\n",
       "      <td>19.673300</td>\n",
       "      <td>20.620400</td>\n",
       "      <td>12.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.874400</td>\n",
       "      <td>1.729748</td>\n",
       "      <td>21.456500</td>\n",
       "      <td>13.572500</td>\n",
       "      <td>19.647900</td>\n",
       "      <td>20.582300</td>\n",
       "      <td>12.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.866700</td>\n",
       "      <td>1.726180</td>\n",
       "      <td>21.435200</td>\n",
       "      <td>13.576900</td>\n",
       "      <td>19.632900</td>\n",
       "      <td>20.558700</td>\n",
       "      <td>12.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.848400</td>\n",
       "      <td>1.723099</td>\n",
       "      <td>21.424200</td>\n",
       "      <td>13.576500</td>\n",
       "      <td>19.643500</td>\n",
       "      <td>20.568100</td>\n",
       "      <td>12.226100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.844100</td>\n",
       "      <td>1.722065</td>\n",
       "      <td>21.437200</td>\n",
       "      <td>13.583700</td>\n",
       "      <td>19.643100</td>\n",
       "      <td>20.571700</td>\n",
       "      <td>12.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.854900</td>\n",
       "      <td>1.720827</td>\n",
       "      <td>21.445300</td>\n",
       "      <td>13.584200</td>\n",
       "      <td>19.648600</td>\n",
       "      <td>20.587100</td>\n",
       "      <td>12.224900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35074, training_loss=1.9800004664906865, metrics={'train_runtime': 8594.5524, 'train_samples_per_second': 34.359, 'train_steps_per_second': 4.296, 'total_flos': 3005267464034304.0, 'train_loss': 1.9800004664906865, 'epoch': 19.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Seq2Seq trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,  # Pre-trained model to fine-tune\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # Training dataset\n",
    "    eval_dataset=tokenized_datasets[\"test\"],  # Evaluation dataset\n",
    "    tokenizer=tokenizer,  # Tokenizer\n",
    "    data_collator=data_collator,  # Data collator for batching\n",
    "    compute_metrics=compute_metrics,  # Metrics computation function\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.01)]  # Stop if no improvement for 5 epochs\n",
    ")\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()\n",
    "\n",
    "# Note: The trained model will be saved in the output_dir specified in training_args\n",
    "# The best model can be loaded for inference using AutoModelForSeq2SeqLM.from_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
